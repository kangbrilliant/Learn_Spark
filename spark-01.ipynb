{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark有那些组件\n",
    "- spark SQL\n",
    "- Spark Streaming : 流失batch处理\n",
    "- Mllib：机器学习库\n",
    "- GraphX：图计算\n",
    "\n",
    "# Spark中driver和executor分别是什么意思?\n",
    "- 一个Spark作业运行时包含一个Driver进程，也是作业的主进程，负责作业的解析/生成Stage并调度Task到Executor上。包含DAGscheduler Taskscheduler\n",
    "\n",
    "- executor：真正执行作业的地方。一个集群一般包含多个executor，每个executor接受driver的命令启动Task，一个executor可以执行一到多个Task\n",
    "\n",
    "\n",
    "# 如何生成RDD?\n",
    "一般三种方式：\n",
    "- 使用程序中的集合创建RDD\n",
    "  - 主要用于进行测试。一般在实际部署到集群运行之前，自己使用集合构造测试数据，来测试后面的spark应用的流程\n",
    "- 使用本地文件创建RDD\n",
    "  - 主要在本地临时性地处理了一些存储了大量数据的文件\n",
    "- 使用HDFS文件创建RDD\n",
    "  ``` \n",
    "  最常用的生产环境处理方式。 主要针对HDFS上存储的大数据，进行离线批处理操作\n",
    "  ```\n",
    "  \n",
    "# Spark中RDD,job,stage,task之间是什么关系?\n",
    "``` https://blog.csdn.net/mys_35088/article/details/80864092 ```\n",
    "- Job: 简单讲就是提交给spark的任务\n",
    "- Stage： 每一个job处理过程要分为的几个阶段\n",
    "``` 在同一个stage 中 所有的task 结束后才能根据DAG依赖执行下一个stage中的task ```\n",
    "- Task： 每一个job处理过程要分为几次任务。\n",
    "``` Task是任务运行的最小单元。最终是要以task为单元运行在executor中。 ```\n",
    "\n",
    "# transformation和action的区别?\n",
    "- transformation：转换操作，返回值还是一个RDD，如map、filter、union\n",
    "- Actions：返回操作结果，或把RDD持久化起来。如count、collect、save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#可在环境变量中进行设置，即PATH中加入如下地址\n",
    "findspark.init(\"C:/Users/Orion/Downloads/spark-2.4.3-bin-hadoop2.7/spark-2.4.3-bin-hadoop2.7\")\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark import SparkConf as conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good good good good good as as as as as am am am am am'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "\n",
    "textFile = sc.textFile('words.txt')\n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(line):\n",
    "     return 'hello' in line\n",
    "filterRDD = textFile.filter(contains)\n",
    "filterRDD.cache()\n",
    "filterRDD.count()\n",
    "\n",
    "counts=textFile.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good good good good good as as as as as am am am am am']\n",
      "[('good', 5), ('as', 5), ('am', 5)]\n"
     ]
    }
   ],
   "source": [
    "print (textFile.collect())\n",
    "print (counts.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
