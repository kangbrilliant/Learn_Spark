{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归\n",
    "- 使用spark完成LR，关于LR不在此赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#可在环境变量中进行设置，即PATH中加入如下地址\n",
    "findspark.init(\"C:/Users/Orion/Downloads/spark-2.4.3-bin-hadoop2.7/spark-2.4.3-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark import sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row,functions\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer,HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel,BinaryLogisticRegressionSummary, LogisticRegression\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "\n",
    "data = sc.textFile(\"iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p)))\n",
    "spark = SparkSession(sc)\n",
    "hasattr(data, \"toDF\")\n",
    "\n",
    "data.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"iris\")\n",
    "df = sql(\"select * from iris where label != 'Iris-setosa'\")\n",
    "rel = df.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").fit(df)\n",
    "#featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_53b988077b38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = df.randomSplit([0.7,0.3])\n",
    "lr = LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "lrPipeline =  Pipeline().setStages([labelIndexer, featureIndexer, lr, labelConverter])\n",
    "lrPipelineModel = lrPipeline.fit(trainingData)\n",
    "\n",
    "lrPredictions = lrPipelineModel.transform(testData)\n",
    "preRel = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in preRel:\n",
    "    print(str(item['label'])+','+str(item['features'])+'-->prob='+str(item['probability'])+',predictedLabel'+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\")\n",
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "print(\"Test Error = \" + str(1.0 - lrAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lrPipelineModel.stages[2]\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients)+\"Intercept: \"+str(lrModel.intercept)+\"numClasses: \"+str(lrModel.numClasses)+\"numFeatures: \"+str(lrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "for item in objectiveHistory:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "- 决策树（decision tree）是一种基本的分类与回归方法，原理在这里不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    " \n",
    "data = SparkContext.textFile(\"iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p))).toDF()\n",
    " \n",
    "data.createOrReplaceTempView(\"iris\")\n",
    " \n",
    "df = spark.sql(\"select * from iris\")\n",
    " \n",
    "rel = df.rdd.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别获取标签列和特征列，进行索引，并进行了重命名。\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    " \n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(df)\n",
    " \n",
    "# 这里我们设置一个labelConverter，目的是把预测的类别重新转化成字符型的。\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "# 接下来，我们把数据集随机分成训练集和测试集，其中训练集占70%。\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel,DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# 训练决策树模型,这里我们可以通过setter的方法来设置决策树的参数，也可以用ParamMap来设置（具体的可以查看spark mllib的官网）。具体的可以设置的参数可以通过explainParams()来获取。\n",
    "dtClassifier = DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "# 在pipeline中进行设置\n",
    "pipelinedClassifier = Pipeline().setStages([labelIndexer, featureIndexer, dtClassifier, labelConverter])\n",
    "# 训练决策树模型\n",
    "modelClassifier = pipelinedClassifier.fit(trainingData)\n",
    "# 进行预测\n",
    "predictionsClassifier = modelClassifier.transform(testData)\n",
    "# 查看部分预测的结果\n",
    "predictionsClassifier.select(\"predictedLabel\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorClassifier = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    " \n",
    "accuracy = evaluatorClassifier.evaluate(predictionsClassifier)\n",
    " \n",
    "print(\"Test Error = \" + str(1.0 - accuracy))\n",
    "Test Error = 0.05882352941176472\n",
    " \n",
    "treeModelClassifier = modelClassifier.stages[2]\n",
    " \n",
    "print(\"Learned classification tree model:\\n\" + str(treeModelClassifier.toDebugString))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel,DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# 训练决策树模型\n",
    "dtRegressor = DecisionTreeRegressor().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "# 在pipeline中进行设置\n",
    "pipelineRegressor = Pipeline().setStages([labelIndexer, featureIndexer, dtRegressor, labelConverter])\n",
    "# 训练决策树模型\n",
    "modelRegressor = pipelineRegressor.fit(trainingData)\n",
    "# 进行预测\n",
    "predictionsRegressor = modelRegressor.transform(testData)\n",
    "# 查看部分预测结果\n",
    "predictionsRegressor.select(\"predictedLabel\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorRegressor = RegressionEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    " \n",
    "rmse = evaluatorRegressor.evaluate(predictionsRegressor)\n",
    " \n",
    "print(\"Root Mean Squared Error (RMSE) on test data = \" +str(rmse))\n",
    "Root Mean Squared Error (RMSE) on test data = 0.24253562503633297\n",
    " \n",
    "treeModelRegressor = modelRegressor.stages[2]\n",
    " \n",
    "print(\"Learned regression tree model:\\n\" + str(treeModelRegressor.toDebugString))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
